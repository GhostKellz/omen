# üöÄ OMEN Quick Start Guide

Get OMEN running in under 5 minutes!

## üì¶ Installation Methods

### Option 1: Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/ghostkellz/omen
cd omen

# Copy environment file
cp .env.example .env

# Edit .env with your API keys
nano .env

# Start OMEN with Redis
docker-compose up -d

# Check status
curl http://localhost:8080/health
```

### Option 2: From Source

```bash
# Install Rust (if not already installed)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Clone and build
git clone https://github.com/ghostkellz/omen
cd omen
cargo build --release

# Copy config
cp .env.example .env
# Edit .env with your API keys

# Run
./target/release/omen serve
```

## üîë Configuration

### Required API Keys

Set these in your `.env` file:

```bash
# At least one provider is required
OMEN_OPENAI_API_KEY=sk-...
OMEN_ANTHROPIC_API_KEY=sk-ant-...
OMEN_GOOGLE_API_KEY=...
OMEN_XAI_API_KEY=xai-...

# For local models (optional)
OMEN_OLLAMA_ENDPOINTS=http://localhost:11434
```

### Provider Priority

OMEN automatically selects providers based on:
- **Local first** for code, regex, tests (if Ollama available)
- **Cloud fallback** for complex reasoning
- **Cost optimization** within budget limits
- **Health checks** ensure availability

## üß™ Test Your Setup

### 1. Check Health
```bash
curl http://localhost:8080/health
```

### 2. List Models
```bash
curl http://localhost:8080/v1/models
```

### 3. Chat Completion
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### 4. Streaming Chat
```bash
curl -N -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [{"role": "user", "content": "Count to 10"}],
    "stream": true
  }'
```

## üîå Integration Examples

### OpenAI Python Client
```python
import openai

client = openai.OpenAI(
    api_key="not-required",  # OMEN handles auth
    base_url="http://localhost:8080/v1"
)

response = client.chat.completions.create(
    model="auto",
    messages=[{"role": "user", "content": "Hello OMEN!"}]
)
print(response.choices[0].message.content)
```

### Zeke.nvim Setup
```lua
-- In your Neovim config
require('zeke').setup({
  provider = {
    openai = {
      base_url = "http://localhost:8080/v1",
      api_key = "not-required"
    }
  }
})
```

### GhostFlow Integration
```yaml
# GhostFlow workflow node
- type: ai_chat
  provider_url: "http://localhost:8080/v1"
  model: "auto"
  messages:
    - role: user
      content: "{{input.query}}"
```

## üè• Health & Monitoring

### Check Provider Status
```bash
curl http://localhost:8080/omen/providers
```

### View Configuration
```bash
curl http://localhost:8080/admin/config
```

### Monitor Usage
```bash
curl http://localhost:8080/admin/usage
```

## üéØ Smart Routing Examples

### Force Local Model
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3:8b",
    "messages": [{"role": "user", "content": "Write a function"}],
    "tags": {"intent": "code", "priority": "low-latency"}
  }'
```

### Force Cloud Model
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-opus-20240229",
    "messages": [{"role": "user", "content": "Complex reasoning task"}],
    "tags": {"intent": "analysis", "priority": "accuracy"}
  }'
```

## üöÄ Next Steps

1. **Add Authentication**: Set `OMEN_MASTER_KEY` for API key protection
2. **Configure Budget**: Set spending limits per provider
3. **Setup Monitoring**: Use Prometheus metrics at `/metrics`
4. **Scale Horizontally**: Run multiple OMEN instances behind a load balancer
5. **Connect Ghost Stack**: Integrate with Zeke, Jarvis, and GhostFlow

## üÜò Troubleshooting

### No Providers Available
- Check API keys in `.env`
- Verify provider health: `curl localhost:8080/omen/providers`
- Check logs: `docker-compose logs omen`

### Ollama Connection Issues
- Ensure Ollama is running: `curl localhost:11434/api/tags`
- Check `OMEN_OLLAMA_ENDPOINTS` configuration
- For Docker: Use `http://ollama:11434` as endpoint

### Model Not Found
- List available models: `curl localhost:8080/v1/models`
- Use `"model": "auto"` for automatic selection
- Check provider-specific model names

## üìö Documentation

- [Architecture Overview](ghostllm-docs/ARCHITECTURE.md)
- [API Reference](ghostllm-docs/API.md)
- [Configuration Guide](ghostllm-docs/guides/configuration.md)
- [Integration Examples](ghostllm-docs/examples/)

---

**Ready to shape the future of AI infrastructure? Welcome to OMEN! üåü**