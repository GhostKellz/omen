# OMEN Docker Compose Configuration
# Complete stack with Redis for caching (optional)

services:
  omen:
    build: .
    image: omen:latest
    container_name: omen
    restart: unless-stopped
    env_file:
      - .env
    environment:
      # Server config
      OMEN_BIND: "0.0.0.0"
      OMEN_PORT: "8080"

      # Database
      OMEN_DB_URL: "sqlite:///app/data/omen.db"
      OMEN_REDIS_URL: "redis://redis:6379"

      # Provider API keys (set these in .env file)
      OMEN_OPENAI_API_KEY: "${OMEN_OPENAI_API_KEY}"
      OMEN_ANTHROPIC_API_KEY: "${OMEN_ANTHROPIC_API_KEY:-}"
      OMEN_GOOGLE_API_KEY: "${OMEN_GOOGLE_API_KEY:-}"
      OMEN_XAI_API_KEY: "${OMEN_XAI_API_KEY:-}"
      OMEN_AZURE_OPENAI_ENDPOINT: "${OMEN_AZURE_OPENAI_ENDPOINT:-}"
      OMEN_AZURE_OPENAI_API_KEY: "${OMEN_AZURE_OPENAI_API_KEY:-}"

      # Ollama endpoints - using existing localhost Ollama (PRODUCTION)
      OMEN_OLLAMA_ENDPOINTS: "${OMEN_OLLAMA_ENDPOINTS}"

      # Routing
      OMEN_ROUTER_PREFER_LOCAL_FOR: "code,regex,tests"
      OMEN_BUDGET_MONTHLY_USD: "150"
    ports:
      - "8080:8080"
    volumes:
      - omen_data:/app/data
      - ./omen.toml:/app/omen.toml:ro
    depends_on:
      - redis
    networks:
      - omen-network
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Allows OMEN to reach localhost:11434
    # Healthcheck disabled - curl not available in minimal image
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 40s

  redis:
    image: redis:7-alpine
    container_name: omen-redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - omen-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # NOTE: Ollama disabled - using existing production Ollama on localhost:11434
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: omen-ollama
  #   restart: unless-stopped
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks:
  #     - omen-network
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   profiles:
  #     - local-ai  # Use 'docker-compose --profile local-ai up' to include

volumes:
  omen_data:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local

networks:
  omen-network:
    driver: bridge